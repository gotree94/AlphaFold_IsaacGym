ë¡œì»¬ LLM ì‹¤í–‰ ë°©ë²•

1. ì£¼ìš” LLM í”„ë ˆì„ì›Œí¬
A. Ollama (ê°€ì¥ ì‰¬ì›€, ì¶”ì²œ!)
ì¥ì : ì„¤ì¹˜ ê°„ë‹¨, ëª¨ë¸ ê´€ë¦¬ ì‰¬ì›€, ìµœì í™” ìš°ìˆ˜
bash# ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl start ollama

# ëª¨ë¸ ì‹¤í–‰ (ì˜ˆ: Llama 3.1 70B)
ollama run llama3.1:70b

# ë‹¤ë¥¸ ì¶”ì²œ ëª¨ë¸ë“¤
ollama run llama3.1:8b          # ë¹ ë¥¸ ì‘ë‹µ
ollama run codellama:34b        # ì½”ë”©
ollama run mistral:7b           # íš¨ìœ¨ì 
ollama run deepseek-coder:33b   # ì½”ë”© íŠ¹í™”
ollama run qwen2.5:72b          # ë‹¤êµ­ì–´, ì¶”ë¡ 

# API ì„œë²„ë¡œ ì‚¬ìš©
# http://localhost:11434 ì—ì„œ OpenAI í˜¸í™˜ API ì œê³µ
B. llama.cpp (ìµœì í™” ìµœê³ )
ì¥ì : C++ ê¸°ë°˜, ë¹ ë¦„, ì–‘ìí™” ìš°ìˆ˜
bash# ì„¤ì¹˜
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make LLAMA_CUDA=1

# ëª¨ë¸ ì‹¤í–‰ (GGUF í¬ë§·)
./main -m models/llama-3.1-70b-Q4_K_M.gguf -n 512 -p "Your prompt"
C. vLLM (ì„œë²„ìš©, ê³ ì„±ëŠ¥)
ì¥ì : ë°°ì¹˜ ì²˜ë¦¬, PagedAttention, ì¶”ë¡  ì†ë„ ìµœê³ 
bash# ì„¤ì¹˜
conda create -n vllm python=3.10 -y
conda activate vllm
pip install vllm

# ëª¨ë¸ ì‹¤í–‰
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.1-70B-Instruct \
    --tensor-parallel-size 1

# API: http://localhost:8000/v1
D. Text Generation WebUI (GUI)
ì¥ì : ì›¹ ì¸í„°í˜ì´ìŠ¤, ChatGPT ìŠ¤íƒ€ì¼
bash# ì„¤ì¹˜
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
./start_linux.sh

# ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:7860 ì ‘ì†
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì±„íŒ… ê°€ëŠ¥
E. LM Studio (ê°€ì¥ ì‚¬ìš©ì ì¹œí™”ì )
ì¥ì : GUI ê¸°ë°˜, ë“œë˜ê·¸ ì•¤ ë“œë¡­, Windows/Mac/Linux
bash# ë‹¤ìš´ë¡œë“œ
wget https://releases.lmstudio.ai/linux/0.2.29/LM-Studio-0.2.29-x86_64.AppImage
chmod +x LM-Studio-0.2.29-x86_64.AppImage
./LM-Studio-0.2.29-x86_64.AppImage

2. RTX 5090 24GBì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª¨ë¸
ëª¨ë¸ í¬ê¸°ë³„ VRAM ì‚¬ìš©ëŸ‰
ëª¨ë¸ í¬ê¸°ì–‘ìí™”VRAM ì‚¬ìš©ì†ë„í’ˆì§ˆ7BFP1614GBë§¤ìš° ë¹ ë¦„ì¢‹ìŒ7BQ4_K_M4GBì´ˆê³ ì†ì–‘í˜¸13BFP1626GBâŒ ë¶ˆê°€-13BQ4_K_M8GBë¹ ë¦„ì¢‹ìŒ34BQ4_K_M20GBë³´í†µë§¤ìš° ì¢‹ìŒ70BQ4_K_M40GBâŒ ë¶ˆê°€-70BQ3_K_M27GBâŒ ë¶ˆê°€-70BQ2_K20GBë³´í†µì–‘í˜¸
ì‹¤ì œ ì¶”ì²œ ëª¨ë¸ (24GB ê¸°ì¤€)
ìµœê³  ì„±ëŠ¥ (í’ˆì§ˆ ìš°ì„ )
bash# Qwen 2.5 32B (Q4) - ë‹¤êµ­ì–´, ìˆ˜í•™, ì½”ë”© ìš°ìˆ˜
ollama run qwen2.5:32b

# DeepSeek Coder V2 16B - ì½”ë”© ìµœê³ 
ollama run deepseek-coder-v2:16b

# Llama 3.1 8B (FP16) - ë¹ ë¥´ê³  ì •í™•
ollama run llama3.1:8b
ê· í˜• (ì†ë„ + í’ˆì§ˆ)
bash# Mistral 7B Instruct - ë²”ìš©
ollama run mistral:7b-instruct

# CodeLlama 34B (Q4) - ì½”ë”©
ollama run codellama:34b

# Phi-3 Medium 14B - íš¨ìœ¨ì 
ollama run phi3:14b
ì†ë„ ìš°ì„ 
bash# Llama 3.2 3B - ì´ˆê³ ì†
ollama run llama3.2:3b

# Gemma 2 9B - ë¹ ë¥¸ ì¶”ë¡ 
ollama run gemma2:9b

# Mistral 7B (Q4) - 2GB VRAM
ollama run mistral:7b-instruct-q4_0

3. RTX 5090 24GB ìµœì  êµ¬ì„±
ì¶”ì²œ #1: Ollama (ì´ˆë³´ì/ì¼ë°˜ ì‚¬ìš©)
bash# ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# ë‹¤ëª©ì  ëª¨ë¸ ì„¤ì¹˜
ollama pull qwen2.5:32b          # ì¶”ë¡ , ë‹¤êµ­ì–´, ìˆ˜í•™
ollama pull deepseek-coder-v2:16b # ì½”ë”©
ollama pull llama3.1:8b          # ë¹ ë¥¸ ì±„íŒ…

# ì‚¬ìš©
ollama run qwen2.5:32b
>>> í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜. ì–‘ìì—­í•™ì„ ì„¤ëª…í•´ë´.
ì¶”ì²œ #2: vLLM (ì„œë²„/API ì‚¬ìš©)
bash# í™˜ê²½ ìƒì„±
conda create -n vllm python=3.10 -y
conda activate vllm
pip install vllm

# Qwen 2.5 32B ì„œë²„ ì‹¤í–‰
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-32B-Instruct \
    --gpu-memory-utilization 0.95 \
    --max-model-len 8192

# OpenAI API ìŠ¤íƒ€ì¼ë¡œ ì‚¬ìš©
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-32B-Instruct",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
ì¶”ì²œ #3: Text Generation WebUI (GUI ì„ í˜¸)
bash# ì„¤ì¹˜
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
./start_linux.sh --api --listen

# ë¸Œë¼ìš°ì €: http://localhost:7860
# Model íƒ­ì—ì„œ ë‹¤ìš´ë¡œë“œ:
# - Qwen/Qwen2.5-32B-Instruct-GGUF
# - TheBloke/deepseek-coder-33B-instruct-GGUF

4. ì„±ëŠ¥ ë¹„êµ (RTX 5090 24GB)
ì¶”ë¡  ì†ë„ (tokens/sec)
ëª¨ë¸ì–‘ìí™”VRAMì†ë„ìš©ë„Llama 3.2 3BQ42GB~150 tok/sì±„íŒ…, ë¹ ë¥¸ ì‘ë‹µLlama 3.1 8BFP1614GB~80 tok/së²”ìš©Mistral 7BQ44GB~100 tok/së²”ìš©Qwen 2.5 14BQ48GB~60 tok/sì¶”ë¡ , ë‹¤êµ­ì–´DeepSeek Coder 16BFP1618GB~50 tok/sì½”ë”©Qwen 2.5 32BQ420GB~30 tok/sê³ í’ˆì§ˆ ì¶”ë¡ CodeLlama 34BQ420GB~25 tok/sì½”ë”©

5. íŠ¹ìˆ˜ ìš©ë„ë³„ ì¶”ì²œ
ìƒëª…ê³¼í•™/ì˜ë£Œ AI í†µí•©
bashconda create -n biomedical-llm python=3.10 -y
conda activate biomedical-llm

# vLLM ì„¤ì¹˜
pip install vllm

# BioGPT ë˜ëŠ” Medical LLM
# Med-PaLM 2ëŠ” ë¹„ê³µê°œì´ì§€ë§Œ ëŒ€ì•ˆ:
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/BioGPT-Large

# ë˜ëŠ” Meditron (ì˜ë£Œ íŠ¹í™”)
ollama pull meditron:70b
ì½”ë”© ì „ìš©
bash# DeepSeek Coder V2 (ìµœê³  ì„±ëŠ¥)
ollama run deepseek-coder-v2:16b

# ë˜ëŠ” CodeLlama
ollama run codellama:34b

# ì‚¬ìš© ì˜ˆ
>>> Pythonìœ¼ë¡œ AlphaFold ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ ì½”ë“œ ì‘ì„±í•´ì¤˜
ë‹¤êµ­ì–´ + ì¶”ë¡ 
bash# Qwen 2.5 (ì¤‘êµ­ì–´, í•œêµ­ì–´, ì˜ì–´ ìš°ìˆ˜)
ollama run qwen2.5:32b

# ë˜ëŠ” Command R+ (RAG ìµœì í™”)
ollama run command-r-plus:104b  # Q2 ì–‘ìí™” í•„ìš”

6. ì„¤ì¹˜ ìš©ëŸ‰
ì†Œí”„íŠ¸ì›¨ì–´
Ollama                  : 500MB
llama.cpp               : 200MB
vLLM                    : 2GB
Text Generation WebUI   : 5GB
ëª¨ë¸ ìš©ëŸ‰ (GGUF ê¸°ì¤€)
7B Q4                   : 4GB
13B Q4                  : 8GB
32B Q4                  : 20GB
70B Q2                  : 26GB
ê¶Œì¥ ë””ìŠ¤í¬

ì†Œí”„íŠ¸ì›¨ì–´: 10GB
ëª¨ë¸ 3-5ê°œ: 50-100GB
ì´: 100GB ì—¬ìœ  ê¶Œì¥


7. ìµœì¢… ì¶”ì²œ êµ¬ì„± (RTX 5090 24GB)
ì˜µì…˜ A: ì˜¬ì¸ì› (Ollama)
bash# ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# 3ê°œ ëª¨ë¸ ì„¤ì¹˜ (ë‹¤ìš©ë„)
ollama pull qwen2.5:32b           # ë©”ì¸ (ì¶”ë¡ , ë‹¤êµ­ì–´)
ollama pull deepseek-coder-v2:16b # ì½”ë”©
ollama pull llama3.2:3b           # ë¹ ë¥¸ ì‘ì—…

# ì´ VRAM: 20GB (qwen), 18GB (deepseek), 2GB (llama3.2)
# ë™ì‹œ ì‹¤í–‰ ë¶ˆê°€, ì „í™˜ ì‚¬ìš©
ì˜µì…˜ B: ì„œë²„ + API (vLLM)
bashconda create -n vllm python=3.10 -y
conda activate vllm
pip install vllm

# ë‹¨ì¼ ëª¨ë¸ ì„œë²„ (ìµœê³  ì„±ëŠ¥)
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-32B-Instruct \
    --gpu-memory-utilization 0.9
ì˜µì…˜ C: GUI (Text Generation WebUI)
bashgit clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
./start_linux.sh

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:7860
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì±„íŒ…

8. í†µí•© í™˜ê²½ êµ¬ì„±
ìƒëª…ê³¼í•™ + LLM í†µí•©
bash# í™˜ê²½ ìƒì„±
conda create -n bio-ai-llm python=3.10 -y
conda activate bio-ai-llm

# PyTorch + CUDA
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y

# ìƒëª…ê³¼í•™ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install monai[all]
pip install bionemo-framework
pip install colabfold[alphafold]

# LLM í”„ë ˆì„ì›Œí¬
pip install vllm
pip install transformers accelerate bitsandbytes

# OllamaëŠ” ì‹œìŠ¤í…œ ë ˆë²¨ë¡œ ë³„ë„ ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

9. RAM + VRAM í†µí•© ì‚¬ìš© (Offloading)
24GB VRAMìœ¼ë¡œ 70B ëª¨ë¸ ì‹¤í–‰í•˜ê¸°:
bash# llama.cpp with GPU offloading
./main \
    -m models/llama-3.1-70b-Q4_K_M.gguf \
    -ngl 35 \  # 35ê°œ ë ˆì´ì–´ë§Œ GPU (ë‚˜ë¨¸ì§€ CPU/RAM)
    -n 512

# ë˜ëŠ” Ollama
OLLAMA_NUM_GPU_LAYERS=35 ollama run llama3.1:70b
VRAM + RAM ì¡°í•©:

VRAM 24GB: 35ê°œ ë ˆì´ì–´
RAM 32GB: ë‚˜ë¨¸ì§€ ë ˆì´ì–´
ì†ë„: ~15 tok/s (ëŠë¦¬ì§€ë§Œ ê°€ëŠ¥)


10. ë¹„ìš© ë° ë¼ì´ì„ ìŠ¤
ë¬´ë£Œ & ì˜¤í”ˆì†ŒìŠ¤

Llama 3.1 (Meta)
Mistral (Mistral AI)
Qwen 2.5 (Alibaba)
DeepSeek Coder (DeepSeek)
Phi-3 (Microsoft)

ìƒì—…ì  ì‚¬ìš©

ëŒ€ë¶€ë¶„ Apache 2.0 ë˜ëŠ” MIT
Llama: ì›” ì‚¬ìš©ì 7ì–µ ë¯¸ë§Œ ë¬´ë£Œ
ëª¨ë‘ ë¡œì»¬ ì‹¤í–‰ ì‹œ API ë¹„ìš© ì—†ìŒ


ìµœì¢… ì¶”ì²œ
ğŸ¥‡ 1ìˆœìœ„: Ollama + Qwen 2.5 32B
bashcurl -fsSL https://ollama.com/install.sh | sh
ollama pull qwen2.5:32b
ollama run qwen2.5:32b
ì´ìœ : ì‰¬ìš´ ì„¤ì¹˜, ë›°ì–´ë‚œ ì„±ëŠ¥, í•œêµ­ì–´ ì§€ì›, ì¶”ë¡  ëŠ¥ë ¥ ìš°ìˆ˜
ğŸ¥ˆ 2ìˆœìœ„: vLLM + DeepSeek Coder V2 16B
bashpip install vllm
python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/DeepSeek-Coder-V2-Instruct-0724
ì´ìœ : ì½”ë”© ì‘ì—…, API ì„œë²„, ë¹ ë¥¸ ì¶”ë¡ 
ğŸ¥‰ 3ìˆœìœ„: Text Generation WebUI
bashgit clone https://github.com/oobabooga/text-generation-webui
./start_linux.sh